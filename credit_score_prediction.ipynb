{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfn5qrUpYxxW",
        "outputId": "72fe876e-06e3-4b6f-8210-57183d3624fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Loading credit score dataset...\n",
            "Dataset loaded: 80000 rows, 29 columns\n",
            "Time to load: 0:00:02.476619\n",
            "Cleaning data...\n",
            "[04:14:23 PM] Performing Data Clean Up\n",
            "Data cleaning completed. Final shape: (80000, 20)\n",
            "Time to clean: 0:01:00.484875\n",
            "Building enhanced neural network...\n",
            "Feature preparation completed. Feature matrix shape: (80000, 33)\n",
            "Training set: (64000, 33), Test set: (16000, 33)\n",
            "Model built successfully. Total parameters: 36483\n",
            "Training model...\n",
            "Epoch 1/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6575 - loss: 0.7433\n",
            "Epoch 2/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6869 - loss: 0.6889\n",
            "Epoch 3/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6898 - loss: 0.6802\n",
            "Epoch 4/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7014 - loss: 0.6622\n",
            "Epoch 5/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7020 - loss: 0.6610\n",
            "Epoch 6/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7094 - loss: 0.6455\n",
            "Epoch 7/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7196 - loss: 0.6337\n",
            "Epoch 8/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7260 - loss: 0.6181\n",
            "Epoch 9/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7330 - loss: 0.6029\n",
            "Epoch 10/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7427 - loss: 0.5848\n",
            "Epoch 11/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.7475 - loss: 0.5729\n",
            "Epoch 12/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7531 - loss: 0.5582\n",
            "Epoch 13/13\n",
            "\u001b[1m3200/3200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7576 - loss: 0.5469\n",
            "500/500 - 1s - 3ms/step - accuracy: 0.7449 - loss: 0.5914\n",
            "Model Accuracy: 74.5%\n",
            "Time to train: 0:01:56.559266\n",
            "Evaluating model...\n",
            "Size of training set: 64000\n",
            "Size of testing set: 16000\n",
            "Generating predictions using Neural Network...\n",
            "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Predictions saved to predictionClassProject9.csv\n",
            "\n",
            "==================================================\n",
            "MODEL PERFORMANCE RESULTS\n",
            "==================================================\n",
            "Accuracy: 74.5%\n",
            "Precision: 73.4%\n",
            "Recall: 75.2%\n",
            "F1-Score: 73.9%\n",
            "\n",
            "Confusion Matrix:\n",
            "Actual Good: [2066   92  724]\n",
            "Actual Poor: [  53 3920  715]\n",
            "Actual Standard: [ 788 1710 5932]\n",
            "\n",
            "Time to generate predictions: 0:00:01.709755\n",
            "Training completed successfully!\n",
            "Final accuracy: 74.5%\n",
            "\n",
            "ðŸŽ‰ Credit Score Prediction Pipeline Complete! ðŸŽ‰\n"
          ]
        }
      ],
      "source": [
        "# Credit Score Prediction Neural Network\n",
        "# Enhanced neural network achieving 74.6% accuracy on 80K+ customer records\n",
        "# Course: CMPS3500 - Class Project\n",
        "# Students: Priscilla Zavala, Jennifer Miranda, Ana Rivera, Francisco Andrade\n",
        "# Date: 12/6/24\n",
        "\n",
        "# Install libraries if needed\n",
        "!pip install tabulate --user\n",
        "\n",
        "# General Packages\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# data handling libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "# visualization libraries\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# extra libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Packages to support NN\n",
        "# sklearn\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# import libraries for timing\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "############################################\n",
        "# Declare vars\n",
        "df = None\n",
        "X_train, X_test, model, encoder, y_test = None, None, None, None, None\n",
        "\n",
        "# set time zone\n",
        "local_time_zone = pytz.timezone('America/Los_Angeles')\n",
        "\n",
        "############################################\n",
        "# Helper functions\n",
        "\n",
        "def current_time_formatted():\n",
        "    return datetime.now(local_time_zone).strftime(\"%I:%M:%S %p\")\n",
        "\n",
        "def describe_numerical_column(series, col_name):\n",
        "    q1, q3 = series.quantile([0.25, 0.75])\n",
        "    IQR = q3 - q1\n",
        "    return {'Min. value': series.min(), 'Outlier lower range': q1 - 1.5 * IQR, 'Outlier upper range': q3 + 1.5 * IQR, 'Max. value': series.max()}\n",
        "\n",
        "def summarize_numerical_column_with_deviation(data, num_col, group_col = 'Customer_ID', absolute_summary = True, median_standardization_summary = False):\n",
        "    '''Summarize the numerical column and its median standardization based on customers using describe_numerical_column function.'''\n",
        "    Summary_dict = {}\n",
        "\n",
        "    if absolute_summary == True:\n",
        "        Summary_dict[num_col] = describe_numerical_column(data[num_col], num_col)\n",
        "\n",
        "    if median_standardization_summary == True:\n",
        "        default_MAD = return_max_MAD(data, num_col, group_col)\n",
        "        num_col_standardization = data.groupby(group_col)[num_col].apply(median_standardization, default_value = default_MAD)\n",
        "        Summary_dict[f'Median standardization of {num_col}'] = describe_numerical_column(num_col_standardization, f'Median standardization of {num_col}')\n",
        "        Summary_dict['Max. MAD'] = default_MAD\n",
        "    return Summary_dict\n",
        "\n",
        "def return_max_MAD(data, num_col, group_col = 'Customer_ID'):\n",
        "    return (data.groupby(group_col)[num_col].agg(lambda x: (x - x.median()).abs().median())).max()\n",
        "\n",
        "def median_standardization(x, default_value):\n",
        "    med = x.median()\n",
        "    abs = (x - med).abs()\n",
        "    MAD = abs.median()\n",
        "    if MAD == 0:\n",
        "        if ((abs == 0).sum() == abs.notnull().sum()): # When MAD is zero and all non-null values are constant in x\n",
        "            return x * 0\n",
        "        else:\n",
        "            return (x - med)/default_value # When MAD is zero but all non-values are not same in x\n",
        "    else:\n",
        "        return (x - med)/MAD # When MAD is non-zero\n",
        "\n",
        "def forward_backward_fill(x):\n",
        "    return x.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "def return_mode_median_filled_int(x):\n",
        "    '''Return back series by filling with mode(in case there is one mode) else fill with integer part of median'''\n",
        "    modes = x.mode()\n",
        "    if len(modes) == 1:\n",
        "        return x.fillna(modes[0])\n",
        "    else:\n",
        "        return x.fillna(int(modes.median()))\n",
        "\n",
        "def fill_month_history(x):\n",
        "    '''Return months filled data for 8-months period'''\n",
        "    first_non_null_idx = x.argmin()\n",
        "    first_non_null_value = x.iloc[first_non_null_idx]\n",
        "    return pd.Series(first_non_null_value + np.array(range(-first_non_null_idx, 8-first_non_null_idx)), index = x.index)\n",
        "\n",
        "def calculate_performance_multiclass(y_true, y_pred):\n",
        "    # Calculates various performance metrics for multiclass classification.\n",
        "    metrics = {}\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['precision'] = precision_score(y_true, y_pred, average='macro')\n",
        "    metrics['recall'] = recall_score(y_true, y_pred, average='macro')\n",
        "    metrics['f1_score'] = f1_score(y_true, y_pred, average='macro')\n",
        "    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
        "    return metrics\n",
        "\n",
        "############################################\n",
        "# Main functions\n",
        "\n",
        "def upload_data(file_):\n",
        "    global df\n",
        "\n",
        "    print(\"Loading credit score dataset...\")\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    current_dir = os.getcwd()\n",
        "    parent_dir = os.path.join(current_dir)\n",
        "    file_path = os.path.join(parent_dir, file_)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if df.empty:\n",
        "            print(\"File is empty.\")\n",
        "            return\n",
        "\n",
        "        columns = df.shape[1]\n",
        "        rows = df.shape[0]\n",
        "\n",
        "        print(f\"Dataset loaded: {rows} rows, {columns} columns\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found.\")\n",
        "        return\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: The file is empty.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    total_time = end_time - start_time\n",
        "    print(f\"Time to load: {total_time}\")\n",
        "\n",
        "def clean_data():\n",
        "    global df\n",
        "    if df is None:\n",
        "        print(\"Error: No data loaded. Please upload data first.\")\n",
        "        return\n",
        "\n",
        "    print(\"Cleaning data...\")\n",
        "    print(f\"[{current_time_formatted()}] Performing Data Clean Up\")\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # Dropping not related columns\n",
        "    columns_to_drop_unrelated = ['Unnamed: 0', 'Month', 'Name', 'SSN',]\n",
        "    df.drop(columns=columns_to_drop_unrelated, inplace=True)\n",
        "\n",
        "    # Dropping columns not in used in this model\n",
        "    columns_to_drop_not_used = [ 'Type_of_Loan', 'Changed_Credit_Limit', 'Total_EMI_per_month',\n",
        "                               'Amount_invested_monthly', 'Monthly_Balance']\n",
        "    df.drop(columns=columns_to_drop_not_used, inplace=True)\n",
        "\n",
        "    ### cleaning age ###\n",
        "    df['Age'] = df['Age'].str.replace('_', '')\n",
        "    df['Age'] = df['Age'].str.replace('#', '', regex=False)\n",
        "    df['Age'] = df['Age'].astype(int)\n",
        "    df['Age'][(df['Age'] > 100) | (df['Age'] <= 0)] = np.nan\n",
        "    df['Age'] =  df.groupby('Customer_ID')['Age'].fillna(method='ffill').fillna(method='bfill').astype(int)\n",
        "\n",
        "    ### cleaning occupation ###\n",
        "    df['Occupation'][df['Occupation'] == '_______'] = np.nan\n",
        "    df['Occupation'] =  df.groupby('Customer_ID')['Occupation'].fillna(method='ffill').fillna(method='bfill')\n",
        "    df['Occupation'] = df['Occupation'].astype(\"string\")\n",
        "\n",
        "    ### annual income ###\n",
        "    df['Annual_Income'] = df['Annual_Income'].str.replace('_', '')\n",
        "    df['Annual_Income'] = df['Annual_Income'].astype(float)\n",
        "    df.loc[df['Annual_Income'] > 180000, 'Annual_Income'] = pd.NA\n",
        "    df['Annual_Income'] = df.groupby('Customer_ID')['Annual_Income'].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    ### monthly inhand salary ###\n",
        "    df['Monthly_Inhand_Salary'] = df.groupby('Customer_ID')['Monthly_Inhand_Salary'].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    ### number of credit cards ###\n",
        "    df.loc[df['Num_Credit_Card'] > 11, 'Num_Credit_Card'] = pd.NA\n",
        "    df['Num_Credit_Card'] = df.groupby('Customer_ID')['Num_Credit_Card'].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    ### interest rate ###\n",
        "    df.loc[df['Interest_Rate'] > 34, 'Interest_Rate'] = pd.NA\n",
        "    df['Interest_Rate'] = df.groupby('Customer_ID')['Interest_Rate'].transform(lambda x: x.median())\n",
        "\n",
        "    ### credit mix ###\n",
        "    df['Credit_Mix'][df['Credit_Mix'] == '_'] = np.nan\n",
        "    df['Credit_Mix'] = df.groupby('Customer_ID')['Credit_Mix'].fillna(method='ffill').fillna(method='bfill')\n",
        "    df['Credit_Mix'] = df['Credit_Mix'].astype(\"string\")\n",
        "\n",
        "    ### credit score ###\n",
        "    df['Credit_Score'] = df['Credit_Score'].astype(\"string\")\n",
        "\n",
        "    ### number of loans ###\n",
        "    df['Num_of_Loan'][df['Num_of_Loan'] == '_'] = np.nan\n",
        "    df['Num_of_Loan'] = df['Num_of_Loan'].astype(str)\n",
        "    df['Num_of_Loan'] = df['Num_of_Loan'].str.replace('_', '', regex=False).replace('', np.nan)\n",
        "    df['Num_of_Loan'] = df['Num_of_Loan'].astype(float)\n",
        "    df['Num_of_Loan'][(df['Num_of_Loan'] > 15) | (df['Num_of_Loan'] <= 0)] = np.nan\n",
        "    df['Num_of_Loan'] = df.groupby('Customer_ID')['Num_of_Loan'].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    ### number of bank accounts ###\n",
        "    df['Num_Bank_Accounts'][df['Num_Bank_Accounts'] < 0] = np.nan\n",
        "    df['Num_Bank_Accounts'][df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts')).abs() > 2] = np.nan\n",
        "    df['Num_Bank_Accounts'] = df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(forward_backward_fill).astype(int)\n",
        "\n",
        "    ### payment of min. amount ###\n",
        "    df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map({'Yes': 1, 'No': 0, 'NM': np.nan})\n",
        "    df['Payment_of_Min_Amount'] = df.groupby('Customer_ID')['Payment_of_Min_Amount'].transform(lambda x: x.fillna(x.mode()[0]))\n",
        "    df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map({1: 'Paid', 0: 'NotPaid'})\n",
        "\n",
        "    ### number of delayed payments ###\n",
        "    df['Num_of_Delayed_Payment'][df['Num_of_Delayed_Payment'] == '_'] = np.nan\n",
        "    df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].astype(str)\n",
        "    df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].str.replace('_', '', regex=False).replace('', np.nan)\n",
        "    df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].astype(float)\n",
        "    summary_num_delayed_payments = summarize_numerical_column_with_deviation(df, 'Num_of_Delayed_Payment', median_standardization_summary = True)\n",
        "    df['Num_of_Delayed_Payment'][(df['Num_of_Delayed_Payment'] > summary_num_delayed_payments['Num_of_Delayed_Payment']['Outlier upper range']) | (df['Num_of_Delayed_Payment'] < 0)] = np.nan\n",
        "    df['Num_of_Delayed_Payment'] = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(return_mode_median_filled_int).astype(int)\n",
        "\n",
        "    ### delay from due date ###\n",
        "    due_date_deviation = df.groupby('Customer_ID')['Delay_from_due_date'].transform(median_standardization, default_value = return_max_MAD(df, 'Delay_from_due_date'))\n",
        "\n",
        "    ### outstanding debt ###\n",
        "    df['Outstanding_Debt'] = df['Outstanding_Debt'].str.replace('_', '')\n",
        "    df['Outstanding_Debt'] = df['Outstanding_Debt'].astype(float)\n",
        "\n",
        "    ### num. of cred inquieries ###\n",
        "    summary_num_credit_inquiries = summarize_numerical_column_with_deviation(df, 'Num_Credit_Inquiries', median_standardization_summary = True)\n",
        "    df['Num_Credit_Inquiries'][(df['Num_Credit_Inquiries'] > summary_num_credit_inquiries['Num_Credit_Inquiries']['Outlier upper range']) | (df['Num_Credit_Inquiries'] < 0)] = np.nan\n",
        "    df['Num_Credit_Inquiries'] = df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(forward_backward_fill).astype(int)\n",
        "\n",
        "    ### credit history age ###\n",
        "    df[['Years', 'Months']] = df['Credit_History_Age'].str.extract('(?P<Years>\\d+) Years and (?P<Months>\\d+) Months').astype(float)\n",
        "    df['Credit_History_Age'] = df['Years'] * 12 + df['Months']\n",
        "    df.drop(columns = ['Years', 'Months'], inplace = True)\n",
        "    df['Credit_History_Age'] = df.groupby('Customer_ID')['Credit_History_Age'].transform(fill_month_history).astype(int)\n",
        "\n",
        "    rows = df.shape[1]\n",
        "    print(f\"Data cleaning completed. Final shape: ({df.shape[0]}, {rows})\")\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    total_time = end_time - start_time\n",
        "    print(f\"Time to clean: {total_time}\")\n",
        "\n",
        "def train_model():\n",
        "    global X_train, X_test, model, encoder, y_test, indices_test\n",
        "\n",
        "    print(\"Building enhanced neural network...\")\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    global df\n",
        "    if df is None:\n",
        "        print(\"Error: No data loaded. Please upload data first.\")\n",
        "        return\n",
        "\n",
        "    ### Feature selection ###\n",
        "    target = ['Credit_Score']\n",
        "    continuous_features = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts',\n",
        "                          'Num_Credit_Card','Interest_Rate', 'Credit_Utilization_Ratio', 'Num_of_Loan',\n",
        "                          'Num_of_Delayed_Payment', 'Delay_from_due_date', 'Outstanding_Debt',\n",
        "                          'Num_Credit_Inquiries', 'Credit_History_Age']\n",
        "    categorical_features = ['Occupation', 'Credit_Mix', 'Payment_of_Min_Amount']\n",
        "\n",
        "    # Encoder for input features\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Encoding categorical features\n",
        "    encoded_features = encoder.fit_transform(df[categorical_features])\n",
        "    encoded_df = pd.DataFrame(encoded_features.toarray(), columns=encoder.get_feature_names_out(categorical_features))\n",
        "    df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "    # Scale continuous features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_continuous_features = scaler.fit_transform(df[continuous_features])\n",
        "    scaled_df = pd.DataFrame(scaled_continuous_features, columns=continuous_features)\n",
        "\n",
        "    # Combine scaled continuous features with encoded categorical features\n",
        "    df_combined = pd.concat([scaled_df, encoded_df], axis=1)\n",
        "    feature_matrix = df_combined.to_numpy()\n",
        "\n",
        "    # Encoding target\n",
        "    encoded_target = encoder.fit_transform(df[target])\n",
        "    encoded_target_df = pd.DataFrame(encoded_target.toarray(), columns=encoder.get_feature_names_out(target))\n",
        "    df = pd.concat([df, encoded_target_df], axis=1)\n",
        "\n",
        "    # Defining data sets\n",
        "    X = feature_matrix\n",
        "    y = encoded_target.toarray()\n",
        "\n",
        "    ### Train ###\n",
        "    # Basic train-test split - 80% training and 20% test\n",
        "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, df.index, test_size=0.20, random_state=42)\n",
        "\n",
        "    print(f\"Feature preparation completed. Feature matrix shape: {feature_matrix.shape}\")\n",
        "    print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "    # Create network topology\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Adding input model - Enhanced from 24 to 32 input layers\n",
        "    model.add(Dense(32, input_dim = X_train.shape[1], activation = 'relu'))\n",
        "\n",
        "    # Adding hidden layers - Optimized architecture\n",
        "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
        "\n",
        "    # Output layer - 3 classes for credit scores\n",
        "    model.add(keras.layers.Dense(3, activation=\"softmax\"))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"Model built successfully. Total parameters:\", model.count_params())\n",
        "    print(\"Training model...\")\n",
        "\n",
        "    # Train the Model - optimized to 13 epochs\n",
        "    model.fit(X_train, y_train, epochs = 13, batch_size = 20)\n",
        "\n",
        "    #Evaluate accuracy\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "    print(f\"Model Accuracy: {test_acc:.1%}\")\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    total_time = end_time - start_time\n",
        "    print(f\"Time to train: {total_time}\")\n",
        "\n",
        "def generate_predictions():\n",
        "    global df, X_train, X_test, model, encoder, y_test, indices_test\n",
        "\n",
        "    print(\"Evaluating model...\")\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    if X_train is not None and X_test is not None:\n",
        "        print(f\"Size of training set: {len(X_train)}\")\n",
        "        print(f\"Size of testing set: {len(X_test)}\")\n",
        "    else:\n",
        "        print(\"X_train or X_test is not initialized. Please train the model first.\")\n",
        "        return\n",
        "\n",
        "    print(\"Generating predictions using Neural Network...\")\n",
        "\n",
        "    # Make Predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Get original labels\n",
        "    y_tested = encoder.inverse_transform(y_test)  # true values\n",
        "    y_predicted = encoder.inverse_transform(predictions)   # predicted values\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    customer_ids_for_predictions = df['ID'][indices_test]\n",
        "    results_df = pd.DataFrame({\n",
        "        'ID': customer_ids_for_predictions.values,\n",
        "        'Credit_Score': y_predicted.flatten()\n",
        "    })\n",
        "    results_df.to_csv('predictionClassProject9.csv', index=False)\n",
        "    print(\"Predictions saved to predictionClassProject9.csv\")\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    performance_metrics = calculate_performance_multiclass(y_tested, y_predicted)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MODEL PERFORMANCE RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Accuracy: {performance_metrics['accuracy']:.1%}\")\n",
        "    print(f\"Precision: {performance_metrics['precision']:.1%}\")\n",
        "    print(f\"Recall: {performance_metrics['recall']:.1%}\")\n",
        "    print(f\"F1-Score: {performance_metrics['f1_score']:.1%}\")\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = performance_metrics['confusion_matrix']\n",
        "    print(\"Actual Good:\", cm[0])\n",
        "    print(\"Actual Poor:\", cm[1])\n",
        "    print(\"Actual Standard:\", cm[2])\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    total_time = end_time - start_time\n",
        "    print(f\"\\nTime to generate predictions: {total_time}\")\n",
        "    print(f\"Training completed successfully!\")\n",
        "    print(f\"Final accuracy: {performance_metrics['accuracy']:.1%}\")\n",
        "\n",
        "############################################\n",
        "# Execute the complete pipeline\n",
        "\n",
        "# Step 1: Upload Data\n",
        "upload_data('credit_score_data.csv')\n",
        "\n",
        "# Step 2: Clean Data\n",
        "clean_data()\n",
        "\n",
        "# Step 3: Train Model\n",
        "train_model()\n",
        "\n",
        "# Step 4: Generate Predictions\n",
        "generate_predictions()\n",
        "\n",
        "print(\"\\nCredit Score Prediction Pipeline Complete!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}